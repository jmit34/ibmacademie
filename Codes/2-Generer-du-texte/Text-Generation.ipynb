{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431a84f4",
   "metadata": {},
   "source": [
    "# Introduction au LLM (petit modèle de langage en réalité) \n",
    "\n",
    "## Générer du texte\n",
    "\n",
    "Gen AI is the buzz ...  GAN's (Générative Adversarial Networks) have been the hype a few years ago (circa 2016) for example, and they provided impressive results (thank to compute resources) see  [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) )\n",
    "\n",
    "Then we had RNN (Recurrent Neural Network) and more recently the \"Transformers\" enabling the production of (LLM : Large Language Models).\n",
    "\n",
    "Let's see here, starting with very basics how an algorithm can generate text. \n",
    "\n",
    "There will be 5 steps : \n",
    "\n",
    "1. random letters \n",
    "2. n-grams\n",
    "3. random words \n",
    "4. words based on their sequence frequencies \n",
    "5. a small size LM build with the Transformer\n",
    " \n",
    "\n",
    "*Jm Torres, August 2023, torrejm@fr.ibm.com*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5847471",
   "metadata": {},
   "source": [
    "# 1. Random letters \n",
    "\n",
    "Without surprise, the result is very bad, but it is very easy to do, so let's do it. \n",
    "\n",
    "Given a list of 26 characters we can genarate a list of characters, and even organize them in words. \n",
    "\n",
    "1. random characters \n",
    "2. random chars grouped in random length words\n",
    "3. same as above, but now the chars are random with a laguage frequency.\n",
    "4. to make things a litte bit better, we can use a realsitic distribution for the words lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 on tire des lettres au hasard\n",
    "from random import randint\n",
    "size = 200 # output size  \n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] \n",
    "\n",
    "for i in range(size):\n",
    "    print(chars[randint(0,len(chars)-1)], end =\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08435d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 on forme des mots de longueur aléatoire\n",
    "# \n",
    "from random import randint\n",
    "size = 20 # taille du texte (en nombre de mots) \n",
    "\n",
    "\n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] \n",
    "text = \"\"\n",
    "for i in range(size): \n",
    "    l = randint(1,10) # max word length = 10 here.\n",
    "    for i in range(l):\n",
    "        text += chars[randint(0,len(chars)-1)]\n",
    "    text += \" \"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca0c5f",
   "metadata": {},
   "source": [
    "On peut faire un peu mieux en utilisant les lettres avec leur fréquence d'apparition dans une langue donnée\n",
    "\n",
    "https://fr.wikipedia.org/wiki/Fr%C3%A9quence_d%27apparition_des_lettres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66077089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3  chars chosen with realistic frequency\n",
    "from random import choices\n",
    "size = 200 # nombre de caractères \n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] # liste des caractères\n",
    "freq = [71,11,32,37,121,11,12,11,66,3,3,50, 26, 64, 50, 25, 6, 61,65,59, 45, 11, 2,4,5,2]\n",
    "\n",
    "\n",
    "# on produit une liste de caractères\n",
    "print(''.join(choices(chars, weights = freq, k = size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut grouper en mots (de longueurs aléatoires)\n",
    "from random import choices\n",
    "size = 20 # nombre de mots\n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] \n",
    "freq = [71,11,32,37,121,11,12,11,66,3,3,50, 26, 64, 50, 25, 6, 61,65,59, 45, 11, 2,4,5,2 ]\n",
    "\n",
    "text = \"\"\n",
    "for i in range(size): \n",
    "    l = randint(1,10) \n",
    "    for i in range(l):\n",
    "        text += choices(chars, freq)[0]\n",
    "    text += \" \"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut trouver la distribution des longueurs de mots dans un texte (le plus grand c'est mieux) \n",
    "\n",
    "import string\n",
    "\n",
    "long = {}\n",
    "\n",
    "with open('ibm.txt','r') as f:\n",
    "    text = f.read()\n",
    "    text = ' '.join(text.split()) # suppress multiple blanc chars\n",
    "    text = text.lower() \n",
    "    text = text.translate(str.maketrans('','', string.punctuation)) # suppresses punctuation\n",
    "    mots = text.split() # make a list of words \n",
    "\n",
    "for mot in mots:\n",
    "    long[len(mot)] = long.get(len(mot),0) + 1\n",
    "    \n",
    "long_mot = []\n",
    "freq_mot = []\n",
    "\n",
    "print(\"  length quantity\")\n",
    "print(\"-------- --------\")\n",
    "for k,v in long.items():\n",
    "    if k < 25:  #longest french word \n",
    "        print(f\"{k:8.0f}  {v:7.0f}\")\n",
    "        long_mot.append(k)\n",
    "        freq_mot.append(v)\n",
    "\n",
    "lf = list(zip(long_mot, freq_mot))\n",
    "ll,ff = [[i for i, j in sorted(lf)],\n",
    "       [j for i, j in sorted(lf)]]\n",
    "\n",
    "print(ll)\n",
    "print(ff)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ll, ff)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aecaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.b\n",
    "from random import choices\n",
    "size = 20 # nombre de mots\n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] \n",
    "freq = [71,11,32,37,121,11,12,11,66,3,3,50, 26, 64, 50, 25, 6, 61,65,59, 45, 11, 2,4,5,2]\n",
    "text = \"\"\n",
    "for i in range(size): \n",
    "    l = choices(ll,ff)[0]\n",
    "    for i in range(l):\n",
    "        text += choices(chars, freq)[0]\n",
    "    text += \" \"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bba1df",
   "metadata": {},
   "source": [
    "Frequencies : \n",
    "\n",
    "English \n",
    "\n",
    "`freq = [82,15,28,42,127,22,20,61,70,2,8,40, 24, 67, 75, 20, 9, 60,63,90,27, 10, 24,2,20,7,]`\n",
    "\n",
    "\n",
    "French : \n",
    "\n",
    "`freq = [71,11,32,37,121,11,12,11,66,3,3,50, 26, 64, 50, 25, 6, 61,65,59, 45, 11, 2,4,5,2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffbf468",
   "metadata": {},
   "source": [
    "... anyways this does not look like a text, by far ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda536e",
   "metadata": {},
   "source": [
    "# 2. n-gram\n",
    "\n",
    "on revient sur les lettres, mais en mieux : notion de contexte (synonyme lointain de prompt dans ce cas)\n",
    "\n",
    "(source https://github.com/alxndrTL/villes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31163f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des données\n",
    "\n",
    "fichier = open('villes.txt')\n",
    "donnees = fichier.read()\n",
    "villes = donnees.replace('\\n', ',').split(',')\n",
    "\n",
    "# préparation des données\n",
    "\n",
    "# on rajoute le token . au début et en fin (fait office de signal de départ et de fin)\n",
    "for ville, i in zip(villes, range(len(villes))):\n",
    "    villes[i] = '.' + ville + '.'\n",
    "\n",
    "# création du vocabulaire\n",
    "vocabulaire = []\n",
    "\n",
    "for ville in villes:\n",
    "    for c in ville:\n",
    "        if c not in vocabulaire:\n",
    "            vocabulaire.append(c)\n",
    "\n",
    "vocabulaire = sorted(vocabulaire)\n",
    "vocabulaire[0] = '.' # 0 est \" \" et 3 est \".\" -> on échange\n",
    "vocabulaire[3] = ' '\n",
    "\n",
    "# pour convertir char <-> int\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "for (c, i) in zip(vocabulaire, range(len(vocabulaire))):\n",
    "    char_to_int[c] = i\n",
    "    int_to_char[i] = c\n",
    "\n",
    "print(vocabulaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bef43d",
   "metadata": {},
   "source": [
    "# unigram ou 1-gram\n",
    "\n",
    "Cette fois, il y a un tout petit \"apprentissage\" : on va compter la fréquence de chaque lettre parmis les 36000 noms de communes.\n",
    "\n",
    "Ensuite, on génère des lettres à partir de ces fréquences.\n",
    "\n",
    "![alex1gram](1gram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du dataset\n",
    "\n",
    "# matrice des données, qui sera de taille (M, 1) pour le unigram\n",
    "# on y place simplement toutes les lettres de toutes les communes\n",
    "\n",
    "X = []\n",
    "\n",
    "for ville in villes:\n",
    "    for char in ville:\n",
    "        X.append([char_to_int[char]])\n",
    "\n",
    "X = torch.asarray(X) # (M, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26014066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modèle uni-gram\n",
    "P = torch.zeros((len(vocabulaire))) # liste de probabilités d'apparition de chaque lettre\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    P[X[i]] += 1 # on augmente le compteur de chaque lettre rencontrée\n",
    "\n",
    "P = P / P.sum(dim=0, keepdim=True) # on transforme les nombres d'apparitions en probabilités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(45)\n",
    "\n",
    "for _ in range(10): # on génère 10 noms de villes\n",
    "    # on génère une lettre tant qu'on ne tombe pas sur \".\", qui signifie la fin\n",
    "    nom = \".\"\n",
    "    while nom[-1] != \".\" or len(nom) == 1:\n",
    "        next_char = int_to_char[torch.multinomial(P, num_samples=1, replacement=True, generator=g).item()]\n",
    "        nom = nom + next_char\n",
    "    print(nom[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c15a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul du coût\n",
    "# cette fois, on parcours toutes les données\n",
    "# rappel de la formule : moyenne(log p_modele(lettre suivante | contexte))\n",
    "# la moyenne se fait sur l'entièreté d'un nom de commune, et sur l'ensemble des noms de communes\n",
    "\n",
    "nll = 0\n",
    "for i in range(X.shape[0]):\n",
    "    nll += torch.log(P[X[i, 0]]) # log p_modele(lettre) (contexte vide ici)\n",
    "-nll/X.shape[0] # moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985bff3",
   "metadata": {},
   "source": [
    "# Bigrams\n",
    "\n",
    "![alex2gram](2gram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14337405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du dataset\n",
    "\n",
    "# matrice des données, qui sera de taille (M, 2) pour le bigram\n",
    "# on y place les lettres deux par deux \n",
    "# par exemple, à partir de paris, on construirait les exemples \".p\", \"pa\", \"ar\", \"ri\", \"is\", \"s.\"\n",
    "\n",
    "X = []\n",
    "\n",
    "for ville in villes:\n",
    "    for ch1, ch2 in zip(ville, ville[1:]):\n",
    "        X.append([char_to_int[ch1], char_to_int[ch2]])\n",
    "\n",
    "X = torch.asarray(X) # (M, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b812c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modèle bigram\n",
    "P = torch.zeros((len(vocabulaire), len(vocabulaire))) # liste de probabilités d'apparition de chaque couple\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    P[X[i, 0], X[i, 1]] += 1 # on augmente le compteur de chaque couple rencontré\n",
    "\n",
    "P = P / P.sum(dim=1, keepdim=True) # on divise pour obtenir des probabilitiés\n",
    "# la dimension 0 correspond à la première lettre de chaque couple (le contexte)\n",
    "# la dimension 1 à la seconde lettre (la lettre prédite)\n",
    "\n",
    "# par exemple, P[char_to_int['a']] correspond à 45 nombres, la distribution de probabilités sur les lettres qui suivent le a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c192c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = torch.Generator().manual_seed(42+4)\n",
    "\n",
    "for _ in range(10):\n",
    "    nom = \".\"\n",
    "    while nom[-1] != \".\" or len(nom) == 1:\n",
    "        last_char = nom[-1]\n",
    "        next_char = int_to_char[torch.multinomial(P[char_to_int[last_char]], num_samples=1, replacement=True, generator=g).item()]\n",
    "        nom = nom + next_char\n",
    "    print(nom[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe870a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul du coût\n",
    "# rappel de la formule : moyenne(log p_modele(lettre suivante | contexte))\n",
    "# la moyenne se fait sur l'entièreté d'un nom de commune, et sur l'ensemble des noms de communes\n",
    "\n",
    "nll = 0\n",
    "for i in range(X.shape[0]):\n",
    "    nll += torch.log(P[X[i, 0], X[i, 1]]) # log p_modele(lettre | contexte) avec contexte = X[i, 0], lettre = X[i, 1]\n",
    "-nll/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f784a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Nombre de paramètres : {len(vocabulaire)**3}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fbd46",
   "metadata": {},
   "source": [
    "# Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on rajoute le token . au début et en fin\n",
    "for ville, i in zip(villes, range(len(villes))):\n",
    "    villes[i] = '.' + ville + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du dataset\n",
    "\n",
    "X = [] # taille (M, 3) pour le trigram\n",
    "\n",
    "# pour paris, on construit \"..p\", \".pa\", \"par\", \"ari\", \"ris\", \"is.\", \"s..\"\n",
    "# d'où la nécessite de rajouter un . au début et à la fin\n",
    "\n",
    "for ville in villes:\n",
    "    for ch1, ch2, ch3 in zip(ville, ville[1:], ville[2:]):\n",
    "        X.append([char_to_int[ch1], char_to_int[ch2], char_to_int[ch3]])\n",
    "\n",
    "X = torch.asarray(X) # (M, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f31274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modèle trigram\n",
    "P = torch.zeros((len(vocabulaire), len(vocabulaire), len(vocabulaire))) # une proba pour chaque trio de lettre\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    P[X[i, 0], X[i, 1], X[i, 2]] += 1\n",
    "\n",
    "P = P / P.sum(dim=2, keepdim=True) # la dernière dimension correspond à la prochain lettre, les 2 premières aux lettres de contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = torch.Generator().manual_seed(4354)\n",
    "\n",
    "for _ in range(10):\n",
    "    nom = \"..\"\n",
    "    while nom[-1] != \".\" or len(nom) == 2:\n",
    "        char_moins_1 = nom[-1]\n",
    "        char_moins_2 = nom[-2]\n",
    "\n",
    "        next_char = int_to_char[torch.multinomial(P[char_to_int[char_moins_2], char_to_int[char_moins_1]], num_samples=1, replacement=True, generator=g).item()]\n",
    "        nom = nom + next_char\n",
    "\n",
    "    print(nom[2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "nll = 0\n",
    "for i in range(X.shape[0]):\n",
    "    nll += torch.log(P[X[i, 0], X[i, 1], X[i, 2]]) # log p_modele(lettre | contexte)\n",
    "-nll/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d510608",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Nombre de paramètres : {len(vocabulaire)**4}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed9defb",
   "metadata": {},
   "source": [
    "et ainsi de suite on pourrait faire des 4gram (164 millions de param), et 5gram (7 milliards)...\n",
    "Il faut trouver autre chose : \n",
    "- réseaux de neurones\n",
    "- transformers :\n",
    "\n",
    "![alexnn](nn.png)\n",
    "\n",
    ".\n",
    "\n",
    "![alex2tf](transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9aa92",
   "metadata": {},
   "source": [
    "# 3. Random words : from a graph\n",
    "\n",
    "\n",
    "We will only get a mix of words from the text, will probably not mean anything. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fab607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random words\n",
    "\n",
    "long = 100\n",
    "with open('ibm.txt','r') as f:\n",
    "    text = f.read()\n",
    "    text = ' '.join(text.split()) # suppression des espaces multiple\n",
    "    text = text.lower() # suppression des majuscules\n",
    "    text = text.translate(str.maketrans('','', string.punctuation)) # supprime ponctu.\n",
    "    words = text.split() # fait une liste des mots\n",
    "    # print(words)\n",
    "\n",
    "t = \"\"\n",
    "for i in range(long): \n",
    "    t += words[randint(1,len(words))] + \" \"\n",
    "\n",
    "print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random words WITH text frequencies\n",
    "\n",
    "long = 50\n",
    "\n",
    "with open('ibm.txt','r') as f:\n",
    "    text = f.read()\n",
    "    text = ' '.join(text.split()) # suppression des espaces multiple\n",
    "    text = text.lower() # suppression des majuscules\n",
    "    text = text.translate(str.maketrans('','', string.punctuation)) # supprime ponctu.\n",
    "    words = text.split() # fait une liste des mots\n",
    "    # print(words)\n",
    "\n",
    "    \n",
    "#words = \"bonjour tout le monde comment ca va le monde\".split()\n",
    "# calcul des fréquences des mots dans le texte. \n",
    "fr = {}\n",
    "for word in words:\n",
    "    fr[word] = fr.get(word,0) + 1 \n",
    "\n",
    "#print(fr)    \n",
    "\n",
    "mots = list(fr.keys())\n",
    "freq = list(fr.values())\n",
    "t = \"\"\n",
    "for i in range(long): \n",
    "    t += choices(mots,freq)[0] + \" \"\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13372e",
   "metadata": {},
   "source": [
    "no grammar, no meaning... just random, we need something else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56eb03",
   "metadata": {},
   "source": [
    "# 4. More sophisticated : transform the text into a graph describing the associations between words\n",
    "\n",
    "Generating text using grammatical rules is a hard problem, so other strategies have been designed trying to \"learn\" from the structure in existing text, and imitate. \n",
    "\n",
    "Let's see a simplisitic example. \n",
    "\n",
    "A text will be transformed into a graph (wieghted and oriented graph) : \n",
    "- words are VERTICES of the graph \n",
    "- EDGES are links between vertices (words) : \n",
    "    - the graph is oriented means the edge are directed. (eat will be connected to apple, but not apple to eat) .\n",
    "    - the graph is weighted : they are given a numerical value : in our case this value is telling something about the frequency of one word appearing after another. (eg: the value between \"plane\" and \"fly\", will be higher that the one between \"elephant\" and \"dance\" ).\n",
    "\n",
    "With a large text we can create such a graph  Par exemple : `je vais à la gare, je vais à la maison pour manger des pâtes, il mange des fruits, elle mange des fruits, je chante à la maison, je vais manger des fuits, à la maison je vais dormir` produit un graphe : \n",
    "\n",
    "![le graphe correspondant](graphe.png)\n",
    "\n",
    "\n",
    "When the graph is produced, it can be used to hop from one word to the other in a random manner but not pure random : using the weights of the departing edges as probabilities to go to the next word. \n",
    "\n",
    "We would need a very large text to hope for good result \n",
    "\n",
    "let's do a small one...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce74e90",
   "metadata": {},
   "source": [
    "### a. class definitions (Vertex and Graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c64fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Vertex:\n",
    "    ''' Class for the vertices (representing the words) :\n",
    "        Attributes : \n",
    "        - value (str): the word itself\n",
    "        - adjacent{} : dictionnary of adjacent vertices (adjacent means : there is an edge)\n",
    "            elements of this dict are {vertex_adjacent1:weight1, ... \n",
    "            vertex_adjacentN:weightN}\n",
    "            weights are integer values incremented each time the two words( vertices ) are read\n",
    "            in order in the body of learning\n",
    "        - neighbors[] : liste of adjacent vertices\n",
    "        - neighbors_value[] : liste of adjacent vertices values (words)\n",
    "        - neighbors_weights[] : liste of corresponding edges weights \n",
    "        these lists are extracted from the adjacency dict when the latter is computed. \n",
    "    '''\n",
    "    def __init__(self,value):\n",
    "        self.value = value  # le mot lui même\n",
    "        self.adjacent = {}  # dictionnaire pour les vertex adjacents\n",
    "        self.neighbors = [] # liste des sommets reliés (ce sont des Vertex)\n",
    "        self.neighbors_value = [] # liste des mots des sommmets reliés\n",
    "        self.neighbors_weights = [] # liste des poids des sommets reliés\n",
    "           \n",
    "    def increment_edge(self,vertex):\n",
    "        \"\"\" increments edge weight between self and another vertex\"\"\"\n",
    "        self.adjacent[vertex] = self.adjacent.get(vertex,0) + 1\n",
    "    \n",
    "    def get_adjacent_nodes(self):\n",
    "        \"\"\" returns adjacent vertexes  \"\"\"\n",
    "        return self.adjacent.keys()\n",
    "    \n",
    "    def vertex_probability_map(self):\n",
    "        \"\"\" compute lists : neighbors[] and neighbors_weights[]\n",
    "            These lists will be used to choose the next word in neighbors[] with the weight \n",
    "            from  neighbors_weights[] \n",
    "            the list neighbors_value is used by get_vertex_adjacent_data, for __str__. method \n",
    "            of class Graph \n",
    "        \"\"\"\n",
    "        for (vertex,weight) in self.adjacent.items():\n",
    "            self.neighbors.append(vertex)\n",
    "            self.neighbors_value.append(vertex.value)\n",
    "            self.neighbors_weights.append(weight)\n",
    "            \n",
    "    #def get_vertex_adjacent_data(self):\n",
    "    #    \"\"\" renvoie la liste des adjacents et leurs poids\n",
    "    #        cette fonction n'est utilisée que pour la méthode __str__. \n",
    "    #        dans la classe Graph \"\"\"\n",
    "    #    return self.neighbors_value, self.neighbors_weights\n",
    "        \n",
    "    def next_word(self):\n",
    "        \"\"\" uses random.choices to draw with non uniform random.\n",
    "            random.choices() returns a list of length k, we use k=1 \n",
    "            we want one word, and we extract the vertex [0] \"\"\"\n",
    "        return random.choices(self.neighbors, weights = self.neighbors_weights, k = 1)[0]\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\" this is for being able to use print \"\"\"\n",
    "        return self.value + \" \" +  ' '.join([node.value for node in self.adjacent.keys()])\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        \"\"\" graph is a dict of vertexes : {word:vertex}\"\"\"\n",
    "        self.vertices = {} \n",
    "        \n",
    "    def get_vertex_values(self):\n",
    "        \"\"\" return all vertices of a Graph \"\"\"\n",
    "        return set(self.vertices.keys())\n",
    "    \n",
    "    def add_vertex(self, value):\n",
    "        self.vertices[value] = Vertex(value)\n",
    "        \n",
    "    def get_vertex(self, value):\n",
    "        if value not in self.vertices : \n",
    "            self.add_vertex(value)\n",
    "        return self.vertices[value]\n",
    "    \n",
    "    def get_next_word(self,current_vertex):\n",
    "        return self.vertices[current_vertex.value].next_word()\n",
    "        \n",
    "    def generate_probability_mappings(self):\n",
    "        \"\"\" for each word/vertex we iterate trough the dict  \n",
    "            to build the list of adjacent words with weights\"\"\"\n",
    "        for vertex in self.vertices.values():\n",
    "            vertex.vertex_probability_map()\n",
    "            \n",
    "    def __str__(self):\n",
    "        \"\"\" this is to be able to print a Graph object\"\"\"\n",
    "        p = \"*** beginning of graph ***\"  \n",
    "        for k,v in self.vertices.items():\n",
    "            p+= \"\\n--- word ->\" + k + \" -- possible next(s) ->\" + str(v.neighbors_value) + \\\n",
    "            \" -- wieght ->\" + str(v.neighbors_weights) + \"\\n\"\n",
    "            p+= \"------\"\n",
    "        p += \"***  end of graph   ***\"\n",
    "        return p\n",
    "    \n",
    "    #def __rts__(self):\n",
    "    #    \"\"\" ma fonction print pour un objet de cette classe\"\"\"\n",
    "    #    p = \"*** début graphe *** \\n\"  \n",
    "    #    for e in self.get_vertex_values(): \n",
    "    #        p+= e + \"->>\" + str(g.get_vertex(e).get_vertex_adjacent_data()) + \"\\n\"\n",
    "    #    p += \"***  fin graphe  ***\"\n",
    "    #    return p\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10079120",
   "metadata": {},
   "source": [
    "### b. graph construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bb72e",
   "metadata": {},
   "source": [
    "1. words extraction\n",
    "2. graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405641e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # to suppress punctuation \n",
    "\n",
    "def get_words_from_text(text_path):\n",
    "    \"\"\" text load & transform: multiple spaces are suppressed, lower cases only, no punctuation.\"\"\"\n",
    "    with open(text_path,'r') as f:\n",
    "        text = f.read()\n",
    "        text = ' '.join(text.split()) # suppression des espaces multiple\n",
    "        text = text.lower() # suppression des majuscules\n",
    "        text = text.translate(str.maketrans('','', string.punctuation)) # supprime ponctu.\n",
    "        words = text.split() # fait une liste des mots\n",
    "        return words\n",
    "        \n",
    "def make_graph(words):\n",
    "    \"\"\" graph construction \"\"\"\n",
    "    g = Graph()\n",
    "    previous_word = None\n",
    "    # for each word from the text, if it is not in the dict \n",
    "    # we add it, and we get the vertex object\n",
    "    for word in words:\n",
    "        word_vertex = g.get_vertex(word)      \n",
    "        # if there is a previous_word, we add a link if not exist yet \n",
    "        # if the edge did exist we increment its value \n",
    "        if previous_word:\n",
    "            previous_word.increment_edge(word_vertex)\n",
    "            \n",
    "        previous_word = word_vertex\n",
    "    \n",
    "    # on a fini de parcourir words, on va générer les probabilités\n",
    "    # c'est à dire : pour chaque mot, on fait la liste des adjacents et la liste \n",
    "    # des poids, pour pouvoir faire le tirage au sort (random.choices)\n",
    "    g.generate_probability_mappings()\n",
    "   \n",
    "    return g\n",
    "\n",
    "def compose(g, words, length=50):\n",
    "    \"\"\" composition du texte, à partir d'un mot au hasard \"\"\"\n",
    "    composition = []\n",
    "    word = g.get_vertex(random.choice(words))\n",
    "    \n",
    "    for _ in range(length):\n",
    "        composition.append(word.value)\n",
    "        word = g.get_next_word(word)\n",
    "        \n",
    "    return composition\n",
    "        \n",
    "# -1- get words from text\n",
    "\n",
    "#words = get_words_from_text(\"hp1.txt\")\n",
    "words = get_words_from_text(\"ibm.txt\")\n",
    "\n",
    "#print(words,'\\n\\n')\n",
    "\n",
    "# -2- make a graph using those words\n",
    "    \n",
    "g = make_graph(words)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51137605",
   "metadata": {},
   "source": [
    "### c. fabrication du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a59e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -3- faire un séquence de N mots parmi words\n",
    "#     avec la structure du graphe G\n",
    "\n",
    "composition = compose(g,words,200)\n",
    "    \n",
    "print(' '.join(composition)) # on transforme la liste en string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964fc7a",
   "metadata": {},
   "source": [
    "### Now it is far better ! but not satisfactory, we use only the liaison from one word to another, this may provide the intuition that we need to use liaisons between more than two words, sequences of words...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1448af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# par curiosité, voici à quoi ressemble le fameux graphe : \n",
    "# pour chaque mot du texte, on voit un couple de listes ([],[]) \n",
    "# la première liste contient l'ensemble des mots qui suivent le mot en question\n",
    "# la seconde contient les quantités de fois ou chacun des mots de la première liste \n",
    "# a suivi. \n",
    "\n",
    "print(g)\n",
    "\n",
    "# on voit aussi que pour beaucoup (quantifier ?) de mots, on n'a collecté qu'un seul suivant, \n",
    "# le modèle est assez pauvre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227543b",
   "metadata": {},
   "source": [
    "# 5. Construction of a small GPT (Generative Pre-trained Transformer) \n",
    "\n",
    "les texte de Sheakespeare : \n",
    "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "Dans les chapitres précédents 1 et 2 : aucune prise en compte des relations entre les éléments (tokens - bouts de textes (1, 2, 3,  n letters ), 3 prise en compte de relation entre 2 voisins stricts). \n",
    "\n",
    "Ici justement l'idée est de prendre en compte les laisons sur un plus grand nombre de tokens consacutifs. C'est ce l'on appelle la notion d'attention. \n",
    "\n",
    "Ce concept a été énoncé dans un article séminal \"Attention is all you need\" en 2017 qui a décrit la notion de \"Transformer\" : https://arxiv.org/abs/1706.03762\n",
    "\n",
    "\n",
    "Le code qui suit, contrairement aux exemples précédent fait appel à des bibliothèques spécialisées (en l'occurence PyTorch) pour construire notre Transformer. Il est issu de :  `Youtube, Andreij Karpathy : Let's build GPT: from scratch, in code, spelled out.` \n",
    "Le début est abordable mais ça se corse beaucoup et je finis par tourner le code et constater que ça fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > \"input.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c270bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f: \n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e92220",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d8663",
   "metadata": {},
   "source": [
    "On va utiliser des tokens de 1 caractère de long, le plus simple possible. (Google utilise le tokenizer SentencePiece (des tokens qui sont des parties de mots, OpenAI avec GTP2 : 50257 différents token (bouts de mots), les séquences encodées sont plus courtes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping char <-> int\n",
    "# imbedding, trivial tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # string to list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # list of int to string\n",
    "\n",
    "print(encode(\"Hello every body\"))\n",
    "print(decode(encode(\"Hello every body\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41479ffa",
   "metadata": {},
   "source": [
    "on va utiliser une structure  PyTorch pour stocker nos séquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# en cas de souci de performance, on pourra restreindre la taille du texte ici \n",
    "# au moment de la définition de data.\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sépare en entrainement et validation datasets:\n",
    "n = int(0.9*len(data)) \n",
    "# 90% en train le reste en val : \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41417a0",
   "metadata": {},
   "source": [
    "On ne va pas mettre l'ensemble du texte dans le transofrmer d'un coup. (Trop difficile à calculer), on y va par chunk (bout), ou bloc.\n",
    "\n",
    "Voyons par exemple le tout premier bloc que l'on peut extraire de notre texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927976a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 \n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  In the block above there are 8 examples :  \n",
    "#  with the context 18, next token is 47 \n",
    "#  with the context 18 and 47 next token is 56 \n",
    "#  with the context 18, 47 and 56  next token is 57\n",
    "#  with the context 18, 47, 56 and 57, next token is 58 \n",
    "# ... \n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size): \n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, then target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bdd1f",
   "metadata": {},
   "source": [
    "Generalizing, with block batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc37934",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # nombre de séquences à traiter \n",
    "block_size = 8 # longueur du contexte\n",
    "\n",
    "# l'array 4 * 8 contient 32 exemple\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\" générate a small batch of data input x, target y\"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # retourne 4 (bath_size) random indexes\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is: {target}\")\n",
    "    \n",
    "    \n",
    "# le tenseur x va rentrer dans le transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b8510",
   "metadata": {},
   "source": [
    "### Ok... on abrège (mis avec de la patience et un peu de courage, il faut voir la vidéo de Andrej Karpathy) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1b51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: train loss 2.6567, val loss 2.6669\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # 5000 c'est mieux mais c'est plus long, mettre 2000 si manque de temps\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b74025",
   "metadata": {},
   "source": [
    "### pas mal après 4 à 5 minutes de calcul sur un laptop standard (maxiter 2000 à 5000 selon le temps) \n",
    "### on obtient environ 200k paramètres ... les \"vrais\" LLM are reaching 100 Billions parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef0c5a",
   "metadata": {},
   "source": [
    "## Sources :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fbb132",
   "metadata": {},
   "source": [
    "\n",
    "- bigrammes : 12 Beginner Python Projects - Coding Course. Chaine Youtube FreeCodeCamp.org : https://www.youtube.com/watch?v=8ext9G7xspg\n",
    "- transformer : Let's build GPT: from scratch, in code, spelled out. Chaine Youtube Andreij Karpathy : https://www.youtube.com/watch?v=kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cec20",
   "metadata": {},
   "source": [
    "### Sources : \n",
    "- partie 2 : Youtube, freecode camp : 12 beginner Python Projects\n",
    "- partie 3 : Youtube, Andreij Karpathy : Let's build GPT: from scratch, in code, spelled out. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
